{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZMfNbHeJvjF",
        "outputId": "5c4aa39c-81d4-4e0e-cd39-d243672b8f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Class Distribution:\n",
            "Class\n",
            "0    763\n",
            "1      9\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "df = pd.read_csv('/content/Creditcard_data.csv')\n",
        "\n",
        "print(\"Original Class Distribution:\")\n",
        "print(df['Class'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
        "\n",
        "df_balanced = pd.concat([X_resampled, y_resampled], axis=1)\n",
        "\n",
        "print(\"Balanced Class Distribution:\")\n",
        "print(df_balanced['Class'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ar4TpD3bKabu",
        "outputId": "e6249cd6-6945-4289-ecc4-a9a8d3fbd89b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced Class Distribution:\n",
            "Class\n",
            "0    763\n",
            "1    763\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z = 1.96\n",
        "p = 0.5\n",
        "E = 0.05\n",
        "\n",
        "n = int((Z**2 * p * (1-p)) / (E**2))\n",
        "\n",
        "print(f\"Calculated Sample Size (n): {n}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEzTuGl9KmAA",
        "outputId": "d2999fad-2178-4f46-9cc8-f52ed57f5166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculated Sample Size (n): 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_random_sampling(df, n):\n",
        "    return df.sample(n=n, random_state=42)\n",
        "\n",
        "def systematic_sampling(df, n):\n",
        "    step = len(df) // n\n",
        "    indices = np.arange(0, len(df), step)[:n]\n",
        "    return df.iloc[indices]\n",
        "\n",
        "def stratified_sampling(df, n):\n",
        "    return df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n // 2)).reset_index(drop=True)\n",
        "\n",
        "def cluster_sampling(df, n):\n",
        "    num_clusters = 20\n",
        "    df_copy = df.copy()\n",
        "    df_copy['cluster'] = np.random.randint(0, num_clusters, size=len(df))\n",
        "\n",
        "    selected_clusters = np.random.choice(num_clusters, size=5, replace=False)\n",
        "    sample = df_copy[df_copy['cluster'].isin(selected_clusters)]\n",
        "\n",
        "    if len(sample) > n:\n",
        "        return sample.sample(n=n, random_state=42).drop('cluster', axis=1)\n",
        "    return sample.drop('cluster', axis=1)\n",
        "\n",
        "def bootstrap_sampling(df, n):\n",
        "    return df.sample(n=n, replace=True, random_state=42)\n",
        "\n",
        "sampling_techniques = {\n",
        "    'Sampling1 (Simple Random)': simple_random_sampling,\n",
        "    'Sampling2 (Systematic)': systematic_sampling,\n",
        "    'Sampling3 (Stratified)': stratified_sampling,\n",
        "    'Sampling4 (Cluster)': cluster_sampling,\n",
        "    'Sampling5 (Bootstrap)': bootstrap_sampling\n",
        "}"
      ],
      "metadata": {
        "id": "UOp9NLprKyha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'M1 (Logistic Regression)': LogisticRegression(max_iter=1000),\n",
        "    'M2 (Decision Tree)': DecisionTreeClassifier(random_state=42),\n",
        "    'M3 (Random Forest)': RandomForestClassifier(random_state=42),\n",
        "    'M4 (SVM)': SVC(),\n",
        "    'M5 (Naive Bayes)': GaussianNB()\n",
        "}\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "df_train = pd.concat([X_train_full, y_train_full], axis=1)\n",
        "\n",
        "results = {model_name: {} for model_name in models.keys()}\n",
        "\n",
        "for sample_name, sample_func in sampling_techniques.items():\n",
        "    sample_data = sample_func(df_train, n)\n",
        "    X_sample = sample_data.drop('Class', axis=1)\n",
        "    y_sample = sample_data['Class']\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_sample, y_sample)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        results[model_name][sample_name] = round(accuracy * 100, 2)\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-INa61uZLA5m",
        "outputId": "3980d52d-8f61-4477-f2d1-c22c9c71cb08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/tmp/ipython-input-2111182086.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  return df.groupby('Class', group_keys=False).apply(lambda x: x.sample(n // 2)).reset_index(drop=True)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame(results).T\n",
        "\n",
        "print(\"Model Accuracy (%) per Sampling Technique:\")\n",
        "print(result_df)\n",
        "\n",
        "print(\"\\n--------------------------------------------------\")\n",
        "print(\"Which sampling technique gives higher accuracy on which model?\")\n",
        "print(\"--------------------------------------------------\")\n",
        "\n",
        "for model in result_df.index:\n",
        "    best_sample = result_df.loc[model].idxmax()\n",
        "    best_acc = result_df.loc[model].max()\n",
        "    print(f\"For {model}, the best technique is {best_sample} with Accuracy: {best_acc}%\")"
      ],
      "metadata": {
        "id": "RkHT7IoLLUr5",
        "outputId": "52b3520b-12c8-419a-ec5d-a97c22492227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (%) per Sampling Technique:\n",
            "                          Sampling1 (Simple Random)  Sampling2 (Systematic)  \\\n",
            "M1 (Logistic Regression)                      92.16                   92.16   \n",
            "M2 (Decision Tree)                            97.71                   97.71   \n",
            "M3 (Random Forest)                            99.67                   99.67   \n",
            "M4 (SVM)                                      64.71                   63.40   \n",
            "M5 (Naive Bayes)                              76.80                   68.30   \n",
            "\n",
            "                          Sampling3 (Stratified)  Sampling4 (Cluster)  \\\n",
            "M1 (Logistic Regression)                   89.87                92.48   \n",
            "M2 (Decision Tree)                         96.08                97.71   \n",
            "M3 (Random Forest)                         99.35                98.69   \n",
            "M4 (SVM)                                   65.69                63.73   \n",
            "M5 (Naive Bayes)                           81.05                67.65   \n",
            "\n",
            "                          Sampling5 (Bootstrap)  \n",
            "M1 (Logistic Regression)                  92.16  \n",
            "M2 (Decision Tree)                        97.39  \n",
            "M3 (Random Forest)                       100.00  \n",
            "M4 (SVM)                                  63.73  \n",
            "M5 (Naive Bayes)                          67.65  \n",
            "\n",
            "--------------------------------------------------\n",
            "Which sampling technique gives higher accuracy on which model?\n",
            "--------------------------------------------------\n",
            "For M1 (Logistic Regression), the best technique is Sampling4 (Cluster) with Accuracy: 92.48%\n",
            "For M2 (Decision Tree), the best technique is Sampling1 (Simple Random) with Accuracy: 97.71%\n",
            "For M3 (Random Forest), the best technique is Sampling5 (Bootstrap) with Accuracy: 100.0%\n",
            "For M4 (SVM), the best technique is Sampling3 (Stratified) with Accuracy: 65.69%\n",
            "For M5 (Naive Bayes), the best technique is Sampling3 (Stratified) with Accuracy: 81.05%\n"
          ]
        }
      ]
    }
  ]
}